{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Beer Company Followers by Tokens and Locations\n",
    "**By:** _Mike Scheibel_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tweepy\n",
    "import re \n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going to clean and tokenize the text files for each beer company**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Amstell Light__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amstell = open(\"AmstelLight_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "amstell_clean = [w for w in amstell.lower().split()] # cast words to lowercase\n",
    "\n",
    "amstell_clean = [w.lower() for w in amstell_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 11696,\n",
       " 'unique_tokens': 5686,\n",
       " 'avg_token_length': 5.992305061559508,\n",
       " 'lexical_diversity': 0.48614911080711354,\n",
       " 'top_10': [('usa', 107),\n",
       "  ('new', 83),\n",
       "  ('golf', 82),\n",
       "  ('ny', 78),\n",
       "  ('love', 71),\n",
       "  ('sports', 66),\n",
       "  ('beer', 59),\n",
       "  ('fl', 46),\n",
       "  ('fan', 45),\n",
       "  ('south', 41)]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(amstell_clean)\n",
    "unique_tokens = len(set(amstell_clean))\n",
    "lex_diversity = len(set(amstell_clean))/len(amstell_clean)\n",
    "avg_token_len = np.mean([len(w) for w in amstell_clean])\n",
    "top_10 = Counter(amstell_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States       22\n",
       "New York, NY        19\n",
       "Chicago, IL         18\n",
       "Florida, USA        12\n",
       "Boston, MA          10\n",
       "USA                  9\n",
       "Chicago              8\n",
       "Atlanta, GA          8\n",
       "Michigan             7\n",
       "Ohio, USA            7\n",
       "California, USA      6\n",
       "Miami, FL            6\n",
       "Indiana, USA         6\n",
       "Charlotte, NC        6\n",
       "New York             6\n",
       "Indianapolis, IN     5\n",
       "Illinois, USA        5\n",
       "Philadelphia, PA     5\n",
       "Brooklyn, NY         5\n",
       "Pittsburgh, PA       5\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd #Using pandas to convert text file to dataframe so I can look up location\n",
    "\n",
    "amstell_df = pd.read_csv(\"AmstelLight_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\") #Using tab delimited \n",
    "#amstell_df\n",
    "\n",
    "amstell_df['location'].value_counts()[:20].sort_values(ascending=False) #Counting up each location from dataframe, \n",
    "#sorting in descending order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amstell Light Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amstell light has one of the higher token length (5.99) and lexical diversity(0.49) in followers descriptions. New York seems like a popular location for this beer, with 19 followers and also NY being the fourth most common work in descritpions. It is also interesting that \"golf\" is one of the more common words in descriptions. Amestell light must be popular among golfers. I also notice that \"south\" is one of the top words and Florida, Atlanta, and Miami are some most common states of Amstell light followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Bud Light__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bud = open(\"budlight_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "bud_clean = [w for w in bud.lower().split()] # cast words to lowercase\n",
    "\n",
    "bud_clean = [w.lower() for w in bud_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 29109,\n",
       " 'unique_tokens': 11924,\n",
       " 'avg_token_length': 5.870967741935484,\n",
       " 'lexical_diversity': 0.4096327596276066,\n",
       " 'top_20': [('usa', 424),\n",
       "  ('love', 259),\n",
       "  ('united', 130),\n",
       "  ('new', 129),\n",
       "  ('ca', 120),\n",
       "  ('life', 109),\n",
       "  ('tx', 105),\n",
       "  ('like', 102),\n",
       "  ('fl', 101),\n",
       "  ('fan', 98)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(bud_clean)\n",
    "unique_tokens = len(set(bud_clean))\n",
    "lex_diversity = len(set(bud_clean))/len(bud_clean)\n",
    "avg_token_len = np.mean([len(w) for w in bud_clean])\n",
    "top_10 = Counter(bud_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States          73\n",
       "California, USA        38\n",
       "Chicago, IL            25\n",
       "Texas, USA             22\n",
       "Los Angeles, CA        18\n",
       "Ohio, USA              17\n",
       "USA                    16\n",
       "Florida, USA           14\n",
       "Michigan, USA          13\n",
       "Tampa, FL              13\n",
       "Virginia, USA          11\n",
       "New York, USA          11\n",
       "Georgia, USA           10\n",
       "Houston, TX            10\n",
       "Missouri, USA          10\n",
       "Orlando, FL            10\n",
       "North Carolina, USA    10\n",
       "Canada                 10\n",
       "Atlanta, GA             9\n",
       "Iowa, USA               9\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bud_df = pd.read_csv(\"budlight_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#bud_df\n",
    "\n",
    "bud_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bud Light Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Bud Light is more popular in the midwest and in California, and less so in east coast states. Florida however has a decent amount of followers of Bud light. The user descriptions are pretty middle of the road, and not much sticks out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Coors Light__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coors = open(\"CoorsLight_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "coors_clean = [w for w in coors.lower().split()] # cast words to lowercase\n",
    "\n",
    "coors_clean = [w.lower() for w in coors_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 29257,\n",
       " 'unique_tokens': 11881,\n",
       " 'avg_token_length': 5.77560925590457,\n",
       " 'lexical_diversity': 0.4060908500529788,\n",
       " 'top_20': [('usa', 427),\n",
       "  ('love', 243),\n",
       "  ('tx', 179),\n",
       "  ('ca', 158),\n",
       "  ('life', 136),\n",
       "  ('like', 117),\n",
       "  ('united', 115),\n",
       "  ('fan', 114),\n",
       "  ('new', 108),\n",
       "  ('sports', 106)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(coors_clean)\n",
    "unique_tokens = len(set(coors_clean))\n",
    "lex_diversity = len(set(coors_clean))/len(coors_clean)\n",
    "avg_token_len = np.mean([len(w) for w in coors_clean])\n",
    "top_10 = Counter(coors_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3067: expected 7 fields, saw 8\\nSkipping line 3679: expected 7 fields, saw 8\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "United States          77\n",
       "Texas, USA             31\n",
       "California, USA        27\n",
       "USA                    27\n",
       "Los Angeles, CA        20\n",
       "Chicago, IL            19\n",
       "Ohio, USA              18\n",
       "Michigan, USA          16\n",
       "North Carolina, USA    16\n",
       "Kansas City, MO        15\n",
       "Tennessee, USA         15\n",
       "San Diego, CA          14\n",
       "Houston, TX            13\n",
       "Omaha, NE              12\n",
       "Florida, USA           12\n",
       "San Antonio, TX        12\n",
       "Las Vegas, NV          12\n",
       "Indiana, USA           11\n",
       "Colorado, USA          11\n",
       "New York, NY           11\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Had an error on one line in this text file so had to skip that one line when converting to data fram\n",
    "coors_df = pd.read_csv(\"CoorsLight_followers.txt\", encoding='utf-8', delimiter=\"\\t\", error_bad_lines=False) \n",
    "\n",
    "# error_bad_lines=False skipped line 3679 which expedted 7 fields, saw 8\n",
    "\n",
    "# remove # from coors_df to see full dataframe\n",
    "#coors_df \n",
    "\n",
    "coors_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coors Light Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coors Light follower descriptions had identical lexical diversity(0.41) to Bud Light, Natural Light and PBR. And like Bud Light, Coors Light has most of its followers living in the Midwest. The one surprise however was that there was only 11 followers that lived in Colorado, which is where Coors Light is brewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Dos Equis__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_equis = open(\"DosEquis_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "dos_equis_clean = [w for w in dos_equis.lower().split()] # cast words to lowercase\n",
    "\n",
    "dos_equis_clean = [w.lower() for w in dos_equis_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 32152,\n",
       " 'unique_tokens': 12761,\n",
       " 'avg_token_length': 5.790837272953471,\n",
       " 'lexical_diversity': 0.3968959940283653,\n",
       " 'top_20': [('usa', 489),\n",
       "  ('tx', 449),\n",
       "  ('love', 219),\n",
       "  ('de', 195),\n",
       "  ('new', 182),\n",
       "  ('ca', 170),\n",
       "  ('texas', 152),\n",
       "  ('life', 142),\n",
       "  ('la', 139),\n",
       "  ('san', 120)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(dos_equis_clean)\n",
    "unique_tokens = len(set(dos_equis_clean))\n",
    "lex_diversity = len(set(dos_equis_clean))/len(dos_equis_clean)\n",
    "avg_token_len = np.mean([len(w) for w in dos_equis_clean])\n",
    "top_10 = Counter(dos_equis_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4958: expected 7 fields, saw 8\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Texas, USA         98\n",
       "United States      96\n",
       "Houston, TX        60\n",
       "Chicago, IL        45\n",
       "Los Angeles, CA    44\n",
       "San Antonio, TX    41\n",
       "Dallas, TX         34\n",
       "California, USA    33\n",
       "USA                31\n",
       "Austin, TX         31\n",
       "Texas              31\n",
       "Phoenix, AZ        27\n",
       "Florida, USA       26\n",
       "New York, NY       23\n",
       "Las Vegas, NV      22\n",
       "México             20\n",
       "Ohio, USA          19\n",
       "Mexico             19\n",
       "New Jersey, USA    17\n",
       "New York, USA      16\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dos_equis_df = pd.read_csv(\"DosEquis_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\", error_bad_lines=False)\n",
    "#dos_equis_df\n",
    "\n",
    "dos_equis_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dos Equis Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like not surprise that most of Dos Equis followers are in Mexico and surrounding states of Texas, Arizona and California. Dos Equis has an overwhelming amount of followers who live in Texas. Mexico also cracks the top location with 20 followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Guinness__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "guinness = open(\"GuinnessGB_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "guinness_clean = [w for w in guinness.lower().split()] # cast words to lowercase\n",
    "\n",
    "guinness_clean = [w.lower() for w in guinness_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 35456,\n",
       " 'unique_tokens': 13538,\n",
       " 'avg_token_length': 6.158506317689531,\n",
       " 'lexical_diversity': 0.3818253610108303,\n",
       " 'top_20': [('england', 864),\n",
       "  ('united', 291),\n",
       "  ('love', 255),\n",
       "  ('kingdom', 248),\n",
       "  ('london', 181),\n",
       "  ('uk', 179),\n",
       "  ('views', 138),\n",
       "  ('fan', 134),\n",
       "  ('rugby', 128),\n",
       "  ('scotland', 127)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(guinness_clean)\n",
    "unique_tokens = len(set(guinness_clean))\n",
    "lex_diversity = len(set(guinness_clean))/len(guinness_clean)\n",
    "avg_token_len = np.mean([len(w) for w in guinness_clean])\n",
    "top_10 = Counter(guinness_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "London, England             121\n",
       "England, United Kingdom     101\n",
       "London                       81\n",
       "United Kingdom               74\n",
       "Lagos, Nigeria               41\n",
       "Manchester, England          36\n",
       "Dublin City, Ireland         28\n",
       "UK                           26\n",
       "Glasgow, Scotland            26\n",
       "Scotland, United Kingdom     25\n",
       "Birmingham, England          24\n",
       "Wales, United Kingdom        23\n",
       "England                      22\n",
       "South East, England          19\n",
       "Nigeria                      18\n",
       "Edinburgh, Scotland          17\n",
       "North West, England          17\n",
       "Liverpool, England           15\n",
       "Leeds, England               14\n",
       "Manchester                   14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guinness_df = pd.read_csv(\"GuinnessGB_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#guinness_df\n",
    "\n",
    "guinness_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guinness Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first surprise about Guinness was that the United States was not one of the top 20 locations. Most of the followers are from some reagion of the England. The second most surprising fact about Guinness followers was that Lagos, Nigeria was right after England. Another observation that is interesting is that Dublin City has 28 followers of Guiness, but no other city in Ireland makes the top 20. That could be because of population however. One more thing I noticed was that Guiness followers descriptions have the highest token length (6.16) but one of the lower lexical diversity count (0.38). This might have to do with the language difference. The final thing was that \"rugby\" was on the top 10 words used, and \"football\" was not. I also notice that Guinness followers seem to have more unique tokens than other beer besides Heineken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Heineken USA__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "heineken = open(\"Heineken_US_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "heineken_clean = [w for w in heineken.lower().split()] # cast words to lowercase\n",
    "\n",
    "heineken_clean = [w.lower() for w in heineken_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 31882,\n",
       " 'unique_tokens': 13872,\n",
       " 'avg_token_length': 5.876544758798068,\n",
       " 'lexical_diversity': 0.435104447650712,\n",
       " 'top_20': [('de', 398),\n",
       "  ('brasil', 383),\n",
       "  ('usa', 298),\n",
       "  ('e', 203),\n",
       "  ('love', 203),\n",
       "  ('new', 184),\n",
       "  ('south', 167),\n",
       "  ('brazil', 144),\n",
       "  ('africa', 135),\n",
       "  ('life', 120)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(heineken_clean)\n",
    "unique_tokens = len(set(heineken_clean))\n",
    "lex_diversity = len(set(heineken_clean))/len(heineken_clean)\n",
    "avg_token_len = np.mean([len(w) for w in heineken_clean])\n",
    "top_10 = Counter(heineken_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                 71\n",
       "Chicago, IL                   38\n",
       "Los Angeles, CA               29\n",
       "California, USA               29\n",
       "Lagos, Nigeria                28\n",
       "Sao Paulo, Brazil             28\n",
       "USA                           25\n",
       "São Paulo, Brasil             25\n",
       "New York, NY                  22\n",
       "Rio de Janeiro, Brazil        21\n",
       "Florida, USA                  21\n",
       "Brasil                        21\n",
       "Texas, USA                    20\n",
       "South Africa                  19\n",
       "Rio de Janeiro, Brasil        18\n",
       "Brazil                        18\n",
       "Johannesburg, South Africa    17\n",
       "Nigeria                       17\n",
       "New Jersey, USA               16\n",
       "Las Vegas, NV                 14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heineken_df = pd.read_csv(\"Heineken_US_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#heineken_df\n",
    "\n",
    "heineken_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heineken Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heineken followers are of course in some of the larger cities in the US. However most of the followers seem to be from Africa and Brasil. This is known from the top words, and the location count. I am a soccer fan and know that Heineken sponsored the last few World Cups, so this might have to do with the popularity in Brazil and Africa who were hosts. Also I noticed that Heineken follower descriptions have the highest lexical diversity (0.44). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Labatt USA__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labatt = open(\"LabattUSA_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "labatt_clean = [w for w in labatt.lower().split()] # cast words to lowercase\n",
    "\n",
    "labatt_clean = [w.lower() for w in labatt_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 32830,\n",
       " 'unique_tokens': 12191,\n",
       " 'avg_token_length': 5.8630520865062445,\n",
       " 'lexical_diversity': 0.3713371915930551,\n",
       " 'top_20': [('ny', 879),\n",
       "  ('usa', 292),\n",
       "  ('new', 271),\n",
       "  ('buffalo', 253),\n",
       "  ('sports', 208),\n",
       "  ('hockey', 185),\n",
       "  ('pa', 167),\n",
       "  ('bills', 147),\n",
       "  ('love', 147),\n",
       "  ('mi', 145)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(labatt_clean)\n",
    "unique_tokens = len(set(labatt_clean))\n",
    "lex_diversity = len(set(labatt_clean))/len(labatt_clean)\n",
    "avg_token_len = np.mean([len(w) for w in labatt_clean])\n",
    "top_10 = Counter(labatt_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buffalo, NY          350\n",
       "United States         78\n",
       "Chicago, IL           40\n",
       "Pittsburgh, PA        38\n",
       "New York, USA         37\n",
       "Michigan, USA         35\n",
       "Pennsylvania, USA     29\n",
       "Rochester, NY         29\n",
       "Philadelphia, PA      28\n",
       "USA                   23\n",
       "New York, NY          21\n",
       "Detroit, MI           18\n",
       "Boston, MA            18\n",
       "Charlotte, NC         18\n",
       "New York              18\n",
       "Cleveland, OH         17\n",
       "Columbus, OH          16\n",
       "Michigan              16\n",
       "Chicago               15\n",
       "Syracuse, NY          14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labatt_df = pd.read_csv(\"LabattUSA_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#labatt_df\n",
    "\n",
    "labatt_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labatt USA Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing I noticed about Labatt USA followers was that they had the lowest lexical diversity (0.37). The next thing I noticed was Canada was not one of the top locations for followers. It could be because this is the Labatt USA twitter account. Buffalo, NY however which is close to Canada has an overwhelming number of followers(350) compared to the rest of the country. When I look at the top words, I see \"Buffalo\" and \"Bills\" on the list which is the NFL team there. 350 followers was triple the size of any other beers top location. It would be interesting to look into why Labatt beer is so popular there. I should also note that \"Hockey\" is a top word in the descriptions as well, so atleast there is some Canadian reference from these followers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Michelob Ultra__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "michelob = open(\"MichelobULTRA_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "michelob_clean = [w for w in michelob.lower().split()] # cast words to lowercase\n",
    "\n",
    "michelob_clean = [w.lower() for w in michelob_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 28316,\n",
       " 'unique_tokens': 11818,\n",
       " 'avg_token_length': 5.824268964543014,\n",
       " 'lexical_diversity': 0.4173612092103404,\n",
       " 'top_20': [('usa', 376),\n",
       "  ('love', 224),\n",
       "  ('ca', 220),\n",
       "  ('tx', 161),\n",
       "  ('life', 143),\n",
       "  ('new', 120),\n",
       "  ('fl', 114),\n",
       "  ('sports', 114),\n",
       "  ('los', 111),\n",
       "  ('la', 105)]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(michelob_clean)\n",
    "unique_tokens = len(set(michelob_clean))\n",
    "lex_diversity = len(set(michelob_clean))/len(michelob_clean)\n",
    "avg_token_len = np.mean([len(w) for w in michelob_clean])\n",
    "top_10 = Counter(michelob_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States          72\n",
       "Los Angeles, CA        62\n",
       "California, USA        38\n",
       "Houston, TX            32\n",
       "USA                    26\n",
       "Texas, USA             26\n",
       "Chicago, IL            23\n",
       "Miami, FL              21\n",
       "Missouri, USA          20\n",
       "Florida, USA           19\n",
       "Boston, MA             15\n",
       "North Carolina, USA    15\n",
       "Atlanta, GA            15\n",
       "Los Angeles            14\n",
       "Austin, TX             14\n",
       "New Jersey, USA        13\n",
       "New York, NY           13\n",
       "Las Vegas, NV          13\n",
       "Phoenix, AZ            13\n",
       "Virginia, USA          12\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michelob_df = pd.read_csv(\"MichelobULTRA_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#michelob_df\n",
    "\n",
    "michelob_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Michelob Ultra Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might just be the most boring beer on this list. None of the top words stand out, besides \"sports\" and the top locations are just the largest cities in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Miller Light__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "miller = open(\"MillerLite_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "miller_clean = [w for w in miller.lower().split()] # cast words to lowercase\n",
    "\n",
    "miller_clean = [w.lower() for w in miller_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 29681,\n",
       " 'unique_tokens': 11677,\n",
       " 'avg_token_length': 5.787001785654122,\n",
       " 'lexical_diversity': 0.3934166638590344,\n",
       " 'top_20': [('usa', 481),\n",
       "  ('love', 239),\n",
       "  ('tx', 159),\n",
       "  ('il', 144),\n",
       "  ('fan', 128),\n",
       "  ('new', 128),\n",
       "  ('life', 124),\n",
       "  ('sports', 122),\n",
       "  ('like', 105),\n",
       "  ('pa', 96)]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(miller_clean)\n",
    "unique_tokens = len(set(miller_clean))\n",
    "lex_diversity = len(set(miller_clean))/len(miller_clean)\n",
    "avg_token_len = np.mean([len(w) for w in miller_clean])\n",
    "top_10 = Counter(miller_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3109: expected 7 fields, saw 8\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chicago, IL            77\n",
       "United States          66\n",
       "Texas, USA             34\n",
       "Ohio, USA              23\n",
       "California, USA        23\n",
       "North Carolina, USA    22\n",
       "Illinois, USA          20\n",
       "Florida, USA           20\n",
       "Pennsylvania, USA      20\n",
       "Michigan, USA          20\n",
       "USA                    19\n",
       "Wisconsin, USA         18\n",
       "Milwaukee, WI          18\n",
       "Los Angeles, CA        18\n",
       "Houston, TX            18\n",
       "Tennessee, USA         17\n",
       "Kentucky, USA          15\n",
       "New York, NY           14\n",
       "Virginia, USA          14\n",
       "Philadelphia, PA       14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miller_df = pd.read_csv(\"MillerLite_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\", error_bad_lines=False)\n",
    "#miller_df\n",
    "\n",
    "miller_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Miller Light Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miller light has one of the lower lexical diversity (0.39)  and average token length (5.79). I also noticed that this beer is very popular in the Midwest states of Illinois, Wisconsin, Ohio and Pennsylvania. Also I notice that North Carolina seems to prefer this beer over the others on the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Modelo USA__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = open(\"ModeloUSA_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "modelo_clean = [w for w in modelo.lower().split()] # cast words to lowercase\n",
    "\n",
    "modelo_clean = [w.lower() for w in modelo_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 31604,\n",
       " 'unique_tokens': 12501,\n",
       " 'avg_token_length': 5.854607011770662,\n",
       " 'lexical_diversity': 0.39555119605113276,\n",
       " 'top_20': [('usa', 441),\n",
       "  ('ca', 397),\n",
       "  ('tx', 263),\n",
       "  ('love', 204),\n",
       "  ('los', 156),\n",
       "  ('il', 147),\n",
       "  ('new', 146),\n",
       "  ('san', 133),\n",
       "  ('de', 118),\n",
       "  ('life', 114)]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(modelo_clean)\n",
    "unique_tokens = len(set(modelo_clean))\n",
    "lex_diversity = len(set(modelo_clean))/len(modelo_clean)\n",
    "avg_token_len = np.mean([len(w) for w in modelo_clean])\n",
    "top_10 = Counter(modelo_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3022: expected 7 fields, saw 8\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chicago, IL            97\n",
       "United States          86\n",
       "California, USA        81\n",
       "Los Angeles, CA        73\n",
       "Houston, TX            39\n",
       "Texas, USA             39\n",
       "Florida, USA           37\n",
       "Dallas, TX             36\n",
       "San Antonio, TX        33\n",
       "Austin, TX             28\n",
       "Las Vegas, NV          25\n",
       "USA                    22\n",
       "New York, NY           20\n",
       "Chicago                20\n",
       "Atlanta, GA            20\n",
       "San Diego, CA          18\n",
       "Phoenix, AZ            17\n",
       "Illinois, USA          16\n",
       "North Carolina, USA    16\n",
       "Denver, CO             16\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_df = pd.read_csv(\"ModeloUSA_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\", error_bad_lines=False)\n",
    "#modelo_df\n",
    "\n",
    "modelo_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo USA Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that pops out to me is that Chicago is the top location for Modelo followers. The states I would expect like California, Texas, Nevada, and Arizona come next. Just strange that Chicago has more followers. It is also interesting that Mexico is not one of the top locations on this list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Natural Light__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nati = open(\"naturallight_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "nati_clean = [w for w in nati.lower().split()] # cast words to lowercase\n",
    "\n",
    "nati_clean = [w.lower() for w in nati_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 27502,\n",
       " 'unique_tokens': 11251,\n",
       " 'avg_token_length': 5.72867427823431,\n",
       " 'lexical_diversity': 0.4090975201803505,\n",
       " 'top_20': [('usa', 475),\n",
       "  ('love', 193),\n",
       "  ('oh', 133),\n",
       "  ('tx', 130),\n",
       "  ('new', 111),\n",
       "  ('pa', 107),\n",
       "  ('university', 105),\n",
       "  ('life', 104),\n",
       "  ('united', 103),\n",
       "  ('states', 94)]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(nati_clean)\n",
    "unique_tokens = len(set(nati_clean))\n",
    "lex_diversity = len(set(nati_clean))/len(nati_clean)\n",
    "avg_token_len = np.mean([len(w) for w in nati_clean])\n",
    "top_10 = Counter(nati_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States          84\n",
       "Ohio, USA              35\n",
       "Texas, USA             35\n",
       "Chicago, IL            31\n",
       "USA                    24\n",
       "Pittsburgh, PA         23\n",
       "Florida, USA           22\n",
       "Illinois, USA          21\n",
       "Cincinnati, OH         19\n",
       "Missouri, USA          19\n",
       "Michigan, USA          19\n",
       "Virginia, USA          18\n",
       "North Carolina, USA    17\n",
       "Indiana, USA           16\n",
       "Los Angeles, CA        14\n",
       "St Louis, MO           14\n",
       "Iowa, USA              14\n",
       "Cleveland, OH          14\n",
       "Philadelphia, PA       13\n",
       "Pennsylvania, USA      13\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nati_df = pd.read_csv(\"naturallight_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#nati_df\n",
    "\n",
    "nati_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural Light Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Light followers have the shortest token lenght(5.73). It was also interesting that not many cities where counted in the top locations, and instead just states. Not sure why that would be. I also noticed that Natural Light followers where the only one on this list to have \"University\" as a top ten word. That makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Pabst Blue Ribbon__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbr = open(\"PabstBlueRibbon_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "pbr_clean = [w for w in pbr.lower().split()] # cast words to lowercase\n",
    "\n",
    "pbr_clean = [w.lower() for w in pbr_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 31554,\n",
       " 'unique_tokens': 12766,\n",
       " 'avg_token_length': 5.907650377131267,\n",
       " 'lexical_diversity': 0.4045762819293909,\n",
       " 'top_20': [('usa', 493),\n",
       "  ('love', 171),\n",
       "  ('beer', 147),\n",
       "  ('new', 140),\n",
       "  ('like', 111),\n",
       "  ('wi', 110),\n",
       "  ('ca', 108),\n",
       "  ('il', 108),\n",
       "  ('sports', 105),\n",
       "  ('pa', 104)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(pbr_clean)\n",
    "unique_tokens = len(set(pbr_clean))\n",
    "lex_diversity = len(set(pbr_clean))/len(pbr_clean)\n",
    "avg_token_len = np.mean([len(w) for w in pbr_clean])\n",
    "top_10 = Counter(pbr_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States          77\n",
       "Chicago, IL            57\n",
       "Wisconsin, USA         38\n",
       "Ohio, USA              31\n",
       "Milwaukee, WI          31\n",
       "Michigan, USA          27\n",
       "California, USA        24\n",
       "Los Angeles, CA        22\n",
       "Pennsylvania, USA      21\n",
       "Texas, USA             20\n",
       "Minneapolis, MN        16\n",
       "USA                    16\n",
       "Chicago                16\n",
       "Pittsburgh, PA         15\n",
       "Philadelphia, PA       15\n",
       "Atlanta, GA            15\n",
       "Minnesota, USA         15\n",
       "Indiana, USA           15\n",
       "San Antonio, TX        14\n",
       "North Carolina, USA    14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbr_df = pd.read_csv(\"PabstBlueRibbon_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#pbr_df\n",
    "\n",
    "pbr_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pabst Blue Ribbon Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing I noticed, was that out of all of these beers, only PBR and Yuengling had \"beer\" as a top ten word in follower descriptions. Another thing I noticed was that PBR has almost twice as many followers in the Midwest states than Miller Light, even though they are both orignially brewed in Wisonsin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Stella Artois__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stella = open(\"StellaArtois_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "stella_clean = [w for w in stella.lower().split()] # cast words to lowercase\n",
    "\n",
    "stella_clean = [w.lower() for w in stella_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 29998,\n",
       " 'unique_tokens': 12821,\n",
       " 'avg_token_length': 5.960997399826655,\n",
       " 'lexical_diversity': 0.42739515967731184,\n",
       " 'top_20': [('usa', 390),\n",
       "  ('love', 342),\n",
       "  ('new', 190),\n",
       "  ('de', 189),\n",
       "  ('ca', 169),\n",
       "  ('life', 135),\n",
       "  ('united', 129),\n",
       "  ('mom', 119),\n",
       "  ('ny', 115),\n",
       "  ('south', 105)]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(stella_clean)\n",
    "unique_tokens = len(set(stella_clean))\n",
    "lex_diversity = len(set(stella_clean))/len(stella_clean)\n",
    "avg_token_len = np.mean([len(w) for w in stella_clean])\n",
    "top_10 = Counter(stella_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_110':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                 86\n",
       "USA                           63\n",
       "Los Angeles, CA               50\n",
       "New York, NY                  40\n",
       "California, USA               36\n",
       "Chicago, IL                   32\n",
       "Florida, USA                  27\n",
       "Texas, USA                    27\n",
       "New York, USA                 18\n",
       "Ohio, USA                     17\n",
       "New Jersey, USA               15\n",
       "North Carolina, USA           14\n",
       "San Francisco, CA             12\n",
       "England, United Kingdom       12\n",
       "Miami, FL                     12\n",
       "Johannesburg, South Africa    11\n",
       "Georgia, USA                  10\n",
       "Boston, MA                    10\n",
       "Atlanta, GA                   10\n",
       "Austin, TX                    10\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stella_df = pd.read_csv(\"StellaArtois_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#stella_df\n",
    "\n",
    "stella_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stella Artois Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stella like Amstell Light have the highest token length(5.96) and lexical diversity(0.43). I also noticed that Stella has a lot of followers in New York and New Jersey. New York followers seem to like the European beers link Stella, Amstell and Heineken better than other beers. It was also interesting that England was the only European location in the top 20 for Stella Artois. Another intersting thing is that \"mom\" was one of the top words from the descriptions. Wondering if Stella is a mom beer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text file for __Yuengling Beer__ followers and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text file for Yuengling followers\n",
    "\n",
    "yuengling = open(\"yuenglingbeer_followers.txt\", encoding='utf-8').read() # reading in text file\n",
    "\n",
    "yuengling_clean = [w for w in yuengling.lower().split()] # cast words to lowercase\n",
    "\n",
    "yuengling_clean = [w.lower() for w in yuengling_clean if w.isalpha() and w not in sw] # remove\n",
    "#stopwords and make sure words are just letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 33261,\n",
       " 'unique_tokens': 12398,\n",
       " 'avg_token_length': 5.914524518204503,\n",
       " 'lexical_diversity': 0.35720513514326085,\n",
       " 'top_20': [('usa', 534),\n",
       "  ('pa', 390),\n",
       "  ('love', 199),\n",
       "  ('beer', 196),\n",
       "  ('sports', 193),\n",
       "  ('fan', 153),\n",
       "  ('new', 149),\n",
       "  ('tx', 145),\n",
       "  ('life', 120),\n",
       "  ('united', 111)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(yuengling_clean)\n",
    "unique_tokens = len(set(yuengling_clean))\n",
    "lex_diversity = len(set(coors_clean))/len(yuengling_clean)\n",
    "avg_token_len = np.mean([len(w) for w in yuengling_clean])\n",
    "top_10 = Counter(yuengling_clean).most_common(10)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_10':top_10}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pennsylvania, USA    100\n",
       "United States         87\n",
       "Philadelphia, PA      48\n",
       "New Jersey, USA       32\n",
       "Pittsburgh, PA        30\n",
       "Texas, USA            29\n",
       "Chicago, IL           25\n",
       "Florida, USA          25\n",
       "Ohio, USA             22\n",
       "Michigan, USA         18\n",
       "Las Vegas, NV         17\n",
       "Pottsville, PA        17\n",
       "Lincoln, NE           16\n",
       "California, USA       16\n",
       "Kansas City, MO       15\n",
       "Nashville, TN         15\n",
       "Omaha, NE             14\n",
       "Pennsylvania          14\n",
       "Los Angeles, CA       14\n",
       "Dallas, TX            14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yuengling_df = pd.read_csv(\"yuenglingbeer_followers.txt\", encoding='utf-8' ,delimiter=\"\\t\")\n",
    "#yuengling_df\n",
    "\n",
    "yuengling_df['location'].value_counts()[:20].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yuengling Beer Token Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last beer on the list Yuengling which has the most followers in Pennsylvania (PA). That makes sense since it is brewed there in Pottsville which has 17 folowers form there. There also seems to be a lot of followers from Nebraska which isn't on the other top locations for the rest of the beers. Also the followers of Yuengling had the lowest lexical diversity (0.36) in there descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
